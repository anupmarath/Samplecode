{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, re, pickle\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from utilities import DeptRenamer, disp_all\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Initial Import to get Databricks to start\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SQ_FT = 10000\n",
    "LATEST_OPEN_DT = '2017-06-30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "dept_renamer = DeptRenamer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT fac_sk, fac_nbr\n",
    "FROM db_enriched.dim_2_fac_view\n",
    "'''\n",
    "\n",
    "sk2nbr_df = spark.sql(query).toPandas().apply(lambda x: pd.to_numeric(x))\n",
    "sk2nbr_df.rename(columns={'fac_nbr' : 'store_id'}, inplace = True)\n",
    "assert(sk2nbr_df.fac_sk.nunique() == sk2nbr_df.shape[0])\n",
    "\n",
    "def fac_sk2id(df_with_sk, drop_sk=True):\n",
    "    global sk2nbr_df\n",
    "    sk2nbr_df_subset = sk2nbr_df[sk2nbr_df['fac_sk'].isin(df_with_sk['fac_sk'])].copy()\n",
    "    \n",
    "    df_with_sk = df_with_sk.merge(sk2nbr_df_subset, how='outer')\n",
    "    df_with_id = df_with_sk[['store_id'] + df_with_sk.drop(columns=['store_id']).columns.tolist()]\n",
    "    \n",
    "    if drop_sk:\n",
    "        df_with_id.drop(columns=['fac_sk'], inplace=True)\n",
    "        \n",
    "    return df_with_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_stores_df = pd.read_excel('../../../data/active_store_list.xlsx', header=3)\n",
    "\n",
    "# mask = active_stores_df['Operating \\nStatus'] == \"1-Open\"\n",
    "# active_stores_df = active_stores_df[mask]\n",
    "\n",
    "# active_store_ids = active_stores_df['Lawson Facility #'].tolist()\n",
    "\n",
    "# with open('../../../data/active_stores_lst.pkl', 'wb') as f:\n",
    "#     pickle.dump(active_store_ids, f)\n",
    "\n",
    "with open('../../../data/active_stores_lst.pkl', 'rb') as f:\n",
    "    active_store_ids = pickle.load(f)\n",
    "\n",
    "with open('../../../data/master_stores_list_for_shrink.pkl', 'rb') as f:\n",
    "    master_stores_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing and Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish Skeleton of Active Stores only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT *\n",
    "FROM db_enriched.lu_store_finance_om\n",
    "WHERE closed_dt=\"9999-12-31\"\n",
    "AND opened_dt <= \"{}\"\n",
    "AND total_building_size_amt >= {}\n",
    "AND store_id IN ({})'''.format(LATEST_OPEN_DT, MIN_SQ_FT, str(active_store_ids)[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(query).toPandas()\n",
    "\n",
    "# only_nan_closed_stores = df['closed_dt'] == 'NaT'\n",
    "# df = df[only_nan_closed_stores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.columns[df.nunique(dropna=False) > 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[col for col in df.columns if \"dw_\" in col], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(r'N/A ?',      np.nan, inplace=True, regex=True)\n",
    "df.replace('          ', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with pd.option_context('max_columns', 1000):\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "numeric_cols = ['store_id', 'store_cd', \n",
    "                'region_id', \n",
    "                'parent_op_area_id', 'parent_op_area_cd', \n",
    "                'op_area_finance_id', 'op_area_finance_cd', \n",
    "                'district_finance_id', 'district_finance_cd',\n",
    "                'store_zip5_id', \n",
    "                'total_building_size_amt',\n",
    "                'total_selling_area_amt',\n",
    "                'banner_id',\n",
    "                'prm_banner_id',\n",
    "                'parent_store_id', \n",
    "                'op_area_id',\n",
    "                'division_id',\n",
    "                'store_voice_phone_nbr',\n",
    "                'store_status_id',\n",
    "                'hours_from_host_tm',\n",
    "                'rog_id']\n",
    "\n",
    "dt_cols = [col for col in df.columns if \"_dt\" in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[numeric_cols] = df[numeric_cols].apply(lambda x: pd.to_numeric(x))\n",
    "\n",
    "df[dt_cols] = df[dt_cols].apply(lambda x: pd.to_datetime(x, errors='coerce'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "code_folding": [
     0,
     5,
     8
    ]
   },
   "source": [
    "def check_if_cols_are_sameish(df, col_1, col_2):\n",
    "    df = df.drop_duplicates(subset=[col_1])\n",
    "    check = { val_1 : val_2 for val_1, val_2 in zip(df[col_1], df[col_2])}\n",
    "    return (df[col_1].replace(check) == df[col_2]).all()\n",
    "\n",
    "def has_prop(set_name, prop):\n",
    "        return len([col for col in df.columns if \"{}_{}\".format(set_name, prop) in col]) >= 1\n",
    "\n",
    "def check_sameishness_of_set(df, set_name):\n",
    "    \n",
    "    has_id = has_prop(set_name, 'id')\n",
    "    has_cd = has_prop(set_name, 'cd')\n",
    "    has_nm = has_prop(set_name, 'nm')\n",
    "\n",
    "    if has_id:\n",
    "        if has_cd:\n",
    "            assert(check_if_cols_are_sameish(df, '{}_id'.format(set_name), '{}_cd'.format(set_name)))\n",
    "        if has_nm:\n",
    "            assert(check_if_cols_are_sameish(df, '{}_id'.format(set_name), '{}_nm'.format(set_name)))\n",
    "    else:\n",
    "        if has_cd:\n",
    "            if has_nm:\n",
    "                assert(check_if_cols_are_sameish(df, '{}_cd'.format(set_name), '{}_nm'.format(set_name)))\n",
    "            else:\n",
    "                print('{} only has \"cd\" trait'.format(set_name))\n",
    "        else:\n",
    "            print('{} only has \"nm\" trait'.format(set_name))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sets = ['store', 'region', 'parent_op_area', 'op_area_finance', 'banner', 'prm_banner']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for set in sets:\n",
    "    check_sameishness_of_set(df, set)\n",
    "    df.drop(columns=['{}_cd'.format(set)], errors='ignore', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(df.store_id.nunique() == df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['store_cd', 'region_cd', 'parent_op_area_cd', 'op_area_finance_cd']\n",
    "df.drop(columns=to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../../data/master_stores_list_for_shrink.pkl', 'wb') as f:\n",
    "#     pickle.dump(master_stores_list_for_shrink, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "df.to_feather('../../../data/temp_dfs/skel_df.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather('../../../data/temp_dfs/skel_df.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_all(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## db_enriched.F_RTL_OPS_WK_FAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT fac_sk, sls_amt, ly_sls_amt, sls_prod_cnt, sls_trips_cnt, wages_amt, labor_manhrs_qty\n",
    "FROM db_enriched.F_RTL_OPS_WK_FAC\n",
    "WHERE wk_id >= \"201800\" AND wk_id < \"201900\"\n",
    "'''\n",
    "\n",
    "rtl_ops_fac_df = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtl_ops_fac_df = rtl_ops_fac_df.toPandas().apply(lambda x: pd.to_numeric(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtl_ops_fac_agg_df = rtl_ops_fac_df.groupby('fac_sk').agg({'sls_amt' : ['sum', 'median', 'std'],\n",
    "                                                           'wages_amt' : ['sum', 'median', 'std'],\n",
    "                                                           'labor_manhrs_qty' : ['sum', 'median', 'std'],\n",
    "                                                           'sls_prod_cnt' : ['sum', 'median', 'std'],\n",
    "                                                           'sls_trips_cnt' : ['sum', 'median', 'std']})\n",
    "rtl_ops_fac_agg_df.columns = [\"_\".join(x) for x in rtl_ops_fac_agg_df.columns.ravel()]\n",
    "rtl_ops_fac_agg_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtl_ops_fac_agg_df.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(fac_sk2id(rtl_ops_fac_agg_df), on='store_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_mask = rtl_ops_fac_agg_df.where(rtl_ops_fac_agg_df==0).dropna(axis=0, how='all').index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtl_ops_fac_agg_zeros_df = rtl_ops_fac_agg_df.iloc[zeros_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.merge(fac_sk2id(rtl_ops_fac_agg_zeros_df, drop_sk=False), how='inner').to_clipboard(index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## db_enriched.F_RTL_OPS_WK_DEPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT *\n",
    "FROM db_enriched.F_RTL_OPS_WK_DEPT\n",
    "WHERE wk_id >= \"201800\" AND wk_id < \"201900\"\n",
    "'''\n",
    "\n",
    "df_sales = spark.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales, Wages, Manhours, Inv_cst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rtl_ops_dept_lvl_data(agg_metric, col_renaming_prefix):\n",
    "    '''\n",
    "    agg_metric: sls_amt, wages_amt, labor_manhrs_qty, inv_cst_amt\n",
    "    '''\n",
    "    \n",
    "    global df_sales\n",
    "    \n",
    "    df = df_sales.groupby('fac_sk').pivot('dept_nbr').sum(agg_metric).toPandas()\n",
    "    df = dept_renamer.rename(df, col_renaming_prefix)\n",
    "    df = df.apply(lambda x: pd.to_numeric(x))\n",
    "    df['{}_store'.format(col_renaming_prefix)] = df.drop(columns=['fac_sk']).sum(axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to Incorporate Total and Department-level Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_sls_df = rtl_ops_dept_lvl_data('sls_amt', 'ttl_sls')\n",
    "dept_wage_df = rtl_ops_dept_lvl_data('wages_amt', 'ttl_wages')\n",
    "dept_hrs_df = rtl_ops_dept_lvl_data('labor_manhrs_qty', 'ttl_hrs')\n",
    "dept_cost_df = rtl_ops_dept_lvl_data('inv_cst_amt', 'ttl_cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_hrs_df.dropna(axis=1, thresh=int(dept_hrs_df.shape[0]*0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_sls_df = df_sales.groupby('fac_sk').pivot('dept_nbr').sum('sls_amt').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_sls_df = dept_renamer.rename(dept_sls_df, 'ttl_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_sls_df = dept_sls_df.apply(lambda x: pd.to_numeric(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dept_sls_df['total_sales'] = dept_sls_df.drop(columns=['fac_sk']).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_sls_df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wage Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_wages_df = rtl_ops_dept_lvl_data('wages_amt', 'ttl_wages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_wages_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_wages_df = df_sales.groupby('fac_sk').pivot('dept_nbr').sum('wages_amt').toPandas()\n",
    "dept_wages_df = dept_renamer.rename(dept_sls_df, 'ttl_sales')\n",
    "dept_wages_df = dept_sls_df.apply(lambda x: pd.to_numeric(x))\n",
    "dept_wages_df['total_sales'] = dept_sls_df.drop(columns=['fac_sk']).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rpt_paxar_upc_wave_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT store_id, dept_id, period_id, total_discount_amt, total_discount_qty, unknown_discount_amt, net_sales\n",
    "FROM db_enriched.rpt_paxar_upc_wave_index\n",
    "WHERE period_id > \"201800\" AND period_id < \"201900\"\n",
    "AND store_id IN ({})\n",
    "'''.format(str(master_stores_list)[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(master_stores_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_pax_df = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_pax_by_dept_df = spk_pax_df.groupby('store_id').pivot('dept_id').sum('total_discount_amt').toPandas()\n",
    "ttl_pax_by_dept_df = dept_renamer.rename(ttl_pax_by_dept_df, 'ttl_pax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_pax_by_dept_df.dropna(axis=1, thresh=.8*ttl_pax_by_dept_df.shape[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_pax_by_dept_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_sum = ['total_discount_amt', 'total_discount_qty', 'unknown_discount_amt', 'net_sales']\n",
    "\n",
    "sum_funcs = [F.sum(col) for col in to_sum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pax_df = spk_pax_df.groupBy(['store_id', 'period_id']).agg(*sum_funcs).toPandas()\n",
    "pax_df = pax_df.apply(lambda x: pd.to_numeric(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pax_df.columns = [re.sub(r'sum\\(([_\\w]+)\\)', '\\g<1>', col) for col in pax_df.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trfms = ['sum', 'std', 'mean', 'median']\n",
    "\n",
    "pax_df = pax_df.groupby('store_id').agg({col : trfms for col in ['total_discount_amt', 'total_discount_qty', 'unknown_discount_amt', 'net_sales']})\n",
    "pax_df.columns = [\"_\".join(x) for x in pax_df.columns.ravel()]\n",
    "pax_df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_disc_amt_col = pax_df['total_discount_amt_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ttl_disc_amt_col.astype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pax_df.sort_values(['store_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pax_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pax_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pax_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pax_df = pax_df.groupBy('store_id').pivot('dept_id').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pax_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pax_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pax_df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pax_df = dept_renamer.rename(pax_df, 'pax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pax_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## db_enriched.by_store_2yr_sd_tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT *\n",
    "FROM db_landing.by_store_by_period_turnover\n",
    "'''\n",
    "\n",
    "tenure_df = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/corbin/Downloads/by_store_by_period_turnover.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.termed_cnt.describe([0.05, 0.25, 0.5, 0.75, 0.9, 0.98])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.termed_cnt.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "365px",
    "width": "527px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
